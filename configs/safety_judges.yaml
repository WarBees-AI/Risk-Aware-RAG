# Safety judges configuration
# This is modular: you can swap judge models (LLM or classifier) later.

safety_judges:
  enabled: true

  # A "judge" can be:
  # - llm: prompt-based scorer
  # - classifier: local model score (future)
  judges:
    introspection_validator:
      type: "llm"
      weight: 1.0
      thresholds:
        pass_min_score: 0.0
      prompt_path: "prompts/introspection.jinja"

    retrieval_gate_validator:
      type: "llm"
      weight: 1.0
      thresholds:
        pass_min_score: 0.0
      prompt_path: "prompts/retrieval_gate.jinja"

    evidence_safety_scorer:
      type: "llm"
      weight: 1.0
      thresholds:
        drop_below: 0.0
      prompt_path: "prompts/evidence_filter.jinja"

    final_answer_safety:
      type: "llm"
      weight: 1.0
      thresholds:
        refuse_if_below: 0.0
      # You can implement a dedicated answer-judge prompt later
      prompt_path: null

  calibration:
    enabled: false
    method: "temperature_scaling"  # placeholder for future
    params: {}

