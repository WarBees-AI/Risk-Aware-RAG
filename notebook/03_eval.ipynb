{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 03 â€” Evaluation\n",
        "\n",
        "This notebook:\n",
        "1) runs evaluation via `scripts/eval_all.py`\n",
        "2) loads/inspects the evaluation report artifacts\n",
        "3) provides basic result summaries\n",
        "\n",
        "Note: full metrics will appear once `rai_rag.eval.run_eval.run_all` is implemented."
      ],
      "metadata": {
        "id": "tGHJJiYctC20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "OUT_DIR = Path(\"runs/eval_001\")\n",
        "cmd = [\n",
        "    sys.executable, \"scripts/eval_all.py\",\n",
        "    \"--config\", \"configs/base.yaml\",\n",
        "    \"--bench_dir\", \"data/benchmarks\",\n",
        "    \"--out_dir\", str(OUT_DIR),\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"Files:\", [p.name for p in OUT_DIR.glob(\"*\")])"
      ],
      "metadata": {
        "id": "zv_YG2HdtEyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_path = OUT_DIR / \"report.json\"\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding=\"utf-8\"))\n",
        "    report\n",
        "else:\n",
        "    print(\"No report.json found. Your internal evaluator may be writing a different output.\")"
      ],
      "metadata": {
        "id": "WFHAJGEdtIbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example: expect future outputs like OUT_DIR / \"results.jsonl\"\n",
        "results_path = OUT_DIR / \"results.jsonl\"\n",
        "if results_path.exists():\n",
        "    rows = []\n",
        "    with results_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            rows.append(json.loads(line))\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.head()\n",
        "else:\n",
        "    print(\"No results.jsonl found (expected once run_all is implemented).\")"
      ],
      "metadata": {
        "id": "xqSAA-HItK6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once you implement evaluation, you might store fields like:\n",
        "# - safe: bool\n",
        "# - helpful: float\n",
        "# - action: Retrieve/Restrict/No-Retrieve\n",
        "# - evidence_kept: int\n",
        "# Here is a robust placeholder summary.\n",
        "\n",
        "if 'df' in globals() and len(df) > 0:\n",
        "    cols = df.columns.tolist()\n",
        "    print(\"Columns:\", cols)\n",
        "\n",
        "    if \"action\" in cols:\n",
        "        print(df[\"action\"].value_counts())\n",
        "\n",
        "    if \"safe\" in cols:\n",
        "        print(\"Safety rate:\", df[\"safe\"].mean())\n",
        "\n",
        "    if \"evidence_kept\" in cols:\n",
        "        print(df[\"evidence_kept\"].describe())\n",
        "else:\n",
        "    print(\"No evaluation dataframe available yet.\")"
      ],
      "metadata": {
        "id": "OtUI3rM-tNiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}